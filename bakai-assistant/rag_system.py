#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
rag_system.py
–°–∏—Å—Ç–µ–º–∞ RAG —Å —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º –¥–ª—è –±–∞–Ω–∫–∞ –ë–∞–∫–∞–π
"""

import re
from typing import List, Dict, Optional, Tuple
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.docstore.document import Document
from config import RAG_CONFIG
from difflib import SequenceMatcher

class BakaiRAG:
    """–°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –±–∞–Ω–∫–∞ –ë–∞–∫–∞–π —Å —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.vectorstore = None
        self.llm = None
        self.embeddings = None
        self.document_count = 0
        self.faq_database = {}  # –ö—ç—à –¥–ª—è —Ç–æ—á–Ω—ã—Ö FAQ
        self._last_search_type = 'no_exact_match'
        self._init_components()
    
    def _init_components(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ RAG —Å–∏—Å—Ç–µ–º—ã"""
        try:
            print("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            self.embeddings = OllamaEmbeddings(model=RAG_CONFIG["embedding_model"])
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
            self.vectorstore = Chroma(
                persist_directory=RAG_CONFIG["chroma_db_path"],
                embedding_function=self.embeddings
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
            self.llm = ChatOllama(
                model=RAG_CONFIG["llm_model"],
                temperature=RAG_CONFIG["temperature"],
                num_predict=RAG_CONFIG["max_tokens"],
                top_p=RAG_CONFIG["top_p"],
                repeat_penalty=RAG_CONFIG["repeat_penalty"]
            )
            
            print("‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            self._validate_database()
            self._build_faq_index()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
            raise
    
    def _validate_database(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            collection = self.vectorstore._collection.get()
            self.document_count = len(collection.get('documents', []))
            print(f"üìä –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç {self.document_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            if self.document_count == 0:
                print("‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã.")
            elif self.document_count < 10:
                print("‚ö†Ô∏è –ú–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _build_faq_index(self) -> None:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ FAQ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        try:
            collection = self.vectorstore._collection.get()
            documents = collection.get('documents', [])
            metadatas = collection.get('metadatas', [])
            
            for doc, metadata in zip(documents, metadatas):
                # –ò—â–µ–º FAQ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                if 'FAQ:' in doc:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç
                    faq_match = re.search(r'FAQ:\s*(.+?)\?\s*\n?–û—Ç–≤–µ—Ç:\s*(.+)', doc, re.DOTALL)
                    if faq_match:
                        question = faq_match.group(1).strip()
                        answer = faq_match.group(2).strip()
                        
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
                        normalized_question = self._normalize_question(question)
                        
                        self.faq_database[normalized_question] = {
                            'original_question': question,
                            'answer': answer,
                            'full_content': doc,
                            'metadata': metadata
                        }
                        
                        print(f"üìù –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω FAQ: {question[:50]}...")
            
            print(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(self.faq_database)} FAQ –∑–∞–ø–∏—Å–µ–π")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å FAQ: {e}")
    
    def _normalize_question(self, question: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        normalized = re.sub(r'\s+', ' ', question.strip())
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = normalized.lower()
        
        return normalized
    
    def search_documents(self, query: str, k: int = None) -> List[Document]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º)"""
        documents, search_type = self.search_documents_with_type(query, k)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ generate_answer
        self._last_search_type = search_type
        
        return documents
    
    def search_documents_with_type(self, query: str, k: int = None) -> Tuple[List[Document], str]:
        """–ü–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –∏–Ω–∞—á–µ - –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫"""
        if k is None:
            k = RAG_CONFIG["search_k"]
        
        try:
            print(f"üîç –ü–æ–∏—Å–∫ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            
            # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ FAQ
            exact_match = self._find_exact_faq_match(query)
            if exact_match:
                print("‚úÖ –ù–∞–π–¥–µ–Ω–æ –¢–û–ß–ù–û–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ FAQ")
                doc = Document(
                    page_content=exact_match['full_content'],
                    metadata=exact_match['metadata'] or {}
                )
                return [doc], 'exact_match'
            
            # –®–∞–≥ 2: –ù–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è - –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
            print("üîç –¢–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç - –≤—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫...")
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Ö–æ–∂–∏–µ FAQ, –ø–æ—Ç–æ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            all_docs = []
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã —Å –Ω–∏–∑–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
            similar_matches = self._find_similar_faq_matches(query, threshold=0.6)
            if similar_matches:
                print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(similar_matches)} –ø–æ—Ö–æ–∂–∏—Ö FAQ")
                for match in similar_matches:
                    doc = Document(
                        page_content=match['full_content'],
                        metadata=match['metadata'] or {}
                    )
                    all_docs.append(doc)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            vector_results = self._enhanced_vector_search(query, k)
            all_docs.extend(vector_results)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            unique_docs = []
            seen_content = set()
            for doc in all_docs:
                if doc.page_content not in seen_content:
                    unique_docs.append(doc)
                    seen_content.add(doc.page_content)
            
            return unique_docs[:k], 'no_exact_match'
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return [], 'error'
    
    def _find_exact_faq_match(self, query: str) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –¢–û–ß–ù–û–ì–û —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ FAQ"""
        
        # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –Ω–æ–º–µ—Ä–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        clean_query = re.sub(r'^\d+\.\s*', '', query.strip())
        clean_query = re.sub(r'\s+', ' ', clean_query)
        
        print(f"üîç –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è: '{clean_query}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π FAQ –Ω–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for normalized_faq, faq_data in self.faq_database.items():
            original_question = faq_data['original_question']
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            clean_original = re.sub(r'\s+', ' ', original_question.strip())
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–µ–≥–∏—Å—Ç—Ä)
            if clean_query.lower() == clean_original.lower():
                print(f"‚úÖ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï –Ω–∞–π–¥–µ–Ω–æ!")
                print(f"   –í–æ–ø—Ä–æ—Å: {original_question}")
                print(f"   –û—Ç–≤–µ—Ç: {faq_data['answer'][:100]}...")
                return faq_data
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (>95%)
            similarity = SequenceMatcher(None, clean_query.lower(), clean_original.lower()).ratio()
            if similarity > 0.95:
                print(f"‚úÖ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï –Ω–∞–π–¥–µ–Ω–æ (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.2f})!")
                print(f"   –í–æ–ø—Ä–æ—Å: {original_question}")
                print(f"   –û—Ç–≤–µ—Ç: {faq_data['answer'][:100]}...")
                return faq_data
        
        print("‚ùå –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return None
    
    def _find_similar_faq_matches(self, query: str, threshold: float = 0.7) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö FAQ —Å –æ—Ü–µ–Ω–∫–æ–π —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        normalized_query = self._normalize_question(query)
        
        similar_matches = []
        
        for faq_question, faq_data in self.faq_database.items():
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = SequenceMatcher(None, normalized_query, faq_question).ratio()
            
            if similarity >= threshold:
                similar_matches.append({
                    'similarity': similarity,
                    'original_question': faq_data['original_question'],
                    'answer': faq_data['answer'],
                    'full_content': faq_data['full_content'],
                    'metadata': faq_data['metadata']
                })
                print(f"   üìã –ü–æ—Ö–æ–∂–∏–π –≤–æ–ø—Ä–æ—Å (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.2f}): {faq_data['original_question']}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        similar_matches.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similar_matches
    
    def _enhanced_vector_search(self, query: str, k: int) -> List[Document]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            keyword_results = self._keyword_search_in_documents(query)
            if keyword_results:
                print(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(keyword_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
                return keyword_results[:k]
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            query_variants = self._generate_query_variants(query)
            
            all_results = []
            
            for variant in query_variants:
                try:
                    results = self.vectorstore.similarity_search_with_score(variant, k=k*2)
                    all_results.extend(results)
                    print(f"   üîç '{variant[:40]}...' ‚Üí {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{variant}': {e}")
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
            unique_docs = {}
            for doc, score in all_results:
                doc_text = doc.page_content
                if doc_text not in unique_docs or unique_docs[doc_text][1] > score:
                    unique_docs[doc_text] = (doc, score)
            
            sorted_results = sorted(unique_docs.values(), key=lambda x: x[1])
            final_docs = [doc for doc, _ in sorted_results[:k]]
            
            return final_docs
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _keyword_search_in_documents(self, query: str) -> List[Document]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            collection = self.vectorstore._collection.get()
            documents = collection.get('documents', [])
            metadatas = collection.get('metadatas', [])
            
            query_lower = query.lower()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            keywords = self._extract_keywords(query_lower)
            
            if not keywords:
                return []
            
            matches = []
            
            for doc, metadata in zip(documents, metadatas):
                doc_lower = doc.lower()
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                match_score = 0
                matched_keywords = []
                
                for keyword in keywords:
                    if keyword in doc_lower:
                        match_score += 1
                        matched_keywords.append(keyword)
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                if match_score > 0:
                    matches.append({
                        'document': Document(page_content=doc, metadata=metadata or {}),
                        'score': match_score,
                        'keywords': matched_keywords,
                        'content_preview': doc[:100] + '...'
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            matches.sort(key=lambda x: x['score'], reverse=True)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            for match in matches[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
                print(f"   üìÑ –ù–∞–π–¥–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç (—Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {match['score']}):")
                print(f"      –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {match['keywords']}")
                print(f"      –ü—Ä–µ–≤—å—é: {match['content_preview']}")
            
            return [match['document'] for match in matches]
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {e}")
            return []
    
    def _extract_keywords(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        stop_words = {
            '–ª–∏', '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç', '–±–∞–Ω–∫', '–±–∞–∫–∞–π', '—É—Å–ª—É–≥–∏', '–≤–∞—à', '–Ω–∞—à', '—ç—Ç–æ', '—á—Ç–æ', '–∫–∞–∫',
            '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–∏–ª–∏', '–∏', '–≤', '–Ω–∞', '—Å', '–¥–ª—è'
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —É–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        words = re.findall(r'\b\w+\b', query.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        special_combinations = {
            '—Å–µ–π—Ñ–æ–≤—ã—Ö —è—á–µ–µ–∫': ['—Å–µ–π—Ñ–æ–≤', '—è—á–µ–µ–∫', '—Å–µ–π—Ñ–æ–≤—ã—Ö', '–¥–µ–ø–æ–∑–∏—Ç–Ω—ã—Ö'],
            '–¥–µ–ø–æ–∑–∏—Ç–Ω—ã—Ö —è—á–µ–µ–∫': ['–¥–µ–ø–æ–∑–∏—Ç–Ω—ã—Ö', '—è—á–µ–µ–∫', '—Å–µ–π—Ñ–æ–≤—ã—Ö'],
            '–±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —è—á–µ–µ–∫': ['–±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö', '—è—á–µ–µ–∫', '—Å–µ–π—Ñ–æ–≤—ã—Ö']
        }
        
        query_text = query.lower()
        for phrase, related_words in special_combinations.items():
            if any(word in query_text for word in phrase.split()):
                keywords.extend(related_words)
        
        return list(set(keywords))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _generate_query_variants(self, query: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        variants = [query, query.lower()]
        
        # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
        clean_query = re.sub(r'^\d+\.\s*', '', query.strip())
        variants.append(clean_query)
        variants.append(clean_query.lower())
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã
        replacements = {
            '—Å–µ–π—Ñ–æ–≤—ã—Ö —è—á–µ–µ–∫': ['–¥–µ–ø–æ–∑–∏—Ç–Ω—ã—Ö —è—á–µ–µ–∫', '–±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —è—á–µ–µ–∫', '—Å–µ–π—Ñ–æ–≤', '—è—á–µ–µ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è'],
            '–∞—Ä–µ–Ω–¥—ã': ['–∞—Ä–µ–Ω–¥–∞', '–ø—Ä–æ–∫–∞—Ç', '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ', '—É—Å–ª—É–≥'],
            '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç': ['–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç', '–µ—Å—Ç—å', '–∏–º–µ–µ—Ç', '–æ–∫–∞–∑—ã–≤–∞–µ—Ç'],
            '—É—Å–ª—É–≥–∏': ['—Å–µ—Ä–≤–∏—Å', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏'],
            '–ø–µ—Ä–µ–≤–µ—Å—Ç–∏': ['–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–ø–µ—Ä–µ—Å–ª–∞—Ç—å', '—Å–¥–µ–ª–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥'],
            '–∫–∞—Ä—Ç—É': ['–∫–∞—Ä—Ç–æ—á–∫—É', '–ø–ª–∞—Å—Ç–∏–∫'],
            'SWIFT': ['—Å–≤–∏—Ñ—Ç', 'swift –ø–µ—Ä–µ–≤–æ–¥', '–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥'],
            '–∫–∞–∫': ['—Å–ø–æ—Å–æ–±', '–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º'],
            '–≥–¥–µ': ['–∞–¥—Ä–µ—Å', '–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ'],
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã': ['—Å–ø—Ä–∞–≤–∫–∏', '–±—É–º–∞–≥–∏', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'],
            '–ª–∏–º–∏—Ç—ã': ['–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è', '–ø—Ä–µ–¥–µ–ª—ã', '–º–∞–∫—Å–∏–º—É–º'],
            '–≤–∞–ª—é—Ç–Ω—ã–µ': ['–≤–∞–ª—é—Ç–Ω—ã—Ö', '–æ–±–º–µ–Ω–Ω—ã—Ö', 'currency']
        }
        
        query_lower = query.lower()
        for original, alternatives in replacements.items():
            if original in query_lower:
                for alt in alternatives:
                    new_variant = query_lower.replace(original, alt)
                    variants.append(new_variant)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if '—Å–µ–π—Ñ' in query_lower:
            variants.extend([
                '—Å–µ–π—Ñ–æ–≤—ã–µ —è—á–µ–π–∫–∏',
                '–±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —è—á–µ–π–∫–∏',
                '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Å–µ–π—Ñ–æ–≤—ã–µ —è—á–µ–π–∫–∏',
                '—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π'
            ])
        
        return list(dict.fromkeys(variants))[:8]  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    
    def generate_answer(self, query: str, documents: List[Document], search_type: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞: –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ —Ç–æ—á–Ω–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è - –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏"""
        
        # –ï—Å–ª–∏ search_type –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ç–∏–ø
        if search_type is None:
            search_type = getattr(self, '_last_search_type', 'no_exact_match')
        
        try:
            if not documents:
                return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –æ—Ñ–∏—Å –±–∞–Ω–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –ë–ï–ó –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if search_type == 'exact_match':
                print("‚úÖ –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –ë–ï–ó –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
                return self._extract_direct_answer(documents[0])
            
            # –í–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            else:
                print("ü§ñ –ù–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                return self._generate_contextual_answer(query, documents)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—É."
    
    def _extract_direct_answer(self, document: Document) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä—è–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏–∑ FAQ –ë–ï–ó –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        content = document.page_content
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç –∏–∑ FAQ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        faq_match = re.search(r'FAQ:\s*(.+?)\?\s*\n?–û—Ç–≤–µ—Ç:\s*(.+)', content, re.DOTALL)
        if faq_match:
            answer = faq_match.group(2).strip()
            print(f"üìã –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {answer[:50]}...")
            return answer
        
        # –ï—Å–ª–∏ –Ω–µ—Ç FAQ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
        return content.strip()
    
    def _generate_contextual_answer(self, query: str, documents: List[Document]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        query_type = self._analyze_query_type(query)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self._build_context(documents)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = self._create_contextual_prompt(context, query, query_type)
        
        print(f"ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Ç–∏–ø–∞: {query_type}")
        
        try:
            response = self.llm.invoke(prompt)
            answer = response.content.strip()
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
            return self._clean_answer(answer)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—É."
    
    def _analyze_query_type(self, query: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['–∫–∞–∫', '–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '—Å–ø–æ—Å–æ–±', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞']):
            return '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'
        elif any(word in query_lower for word in ['–≥–¥–µ', '–∞–¥—Ä–µ—Å', '–Ω–∞—Ö–æ–¥–∏—Ç—Å—è']):
            return '–∞–¥—Ä–µ—Å'
        elif any(word in query_lower for word in ['–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Ç—Ä–µ–±—É—é—Ç—Å—è', '–Ω—É–∂–Ω—ã']):
            return '–¥–æ–∫—É–º–µ–Ω—Ç—ã'
        elif any(word in query_lower for word in ['–ª–∏–º–∏—Ç', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ', '–º–∞–∫—Å–∏–º—É–º']):
            return '–ª–∏–º–∏—Ç—ã'
        elif any(word in query_lower for word in ['—á—Ç–æ —Ç–∞–∫–æ–µ', '—Ä–∞—Å—Å–∫–∞–∂–∏']):
            return '–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ'
        else:
            return '–æ–±—â–∏–π'
    
    def _build_context(self, documents: List[Document]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            content = doc.page_content.strip()
            
            metadata_info = ""
            if hasattr(doc, 'metadata') and doc.metadata:
                metadata = doc.metadata
                if 'type' in metadata:
                    metadata_info = f"[{metadata['type']}] "
            
            context_parts.append(f"–î–æ–∫—É–º–µ–Ω—Ç {i}: {metadata_info}{content}")
        
        return "\n\n".join(context_parts)
    
    def _create_contextual_prompt(self, context: str, query: str, query_type: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        return f"""–¢—ã - –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –±–∞–Ω–∫–∞ –ë–∞–∫–∞–π. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ —Ç–æ—á–Ω—ã–º
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ï—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º

–ö–û–ù–¢–ï–ö–°–¢:
{context}

–í–û–ü–†–û–°: {query}

–û–¢–í–ï–¢:"""
    
    def _clean_answer(self, answer: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Ñ—Ä–∞–∑—ã
        unwanted_phrases = [
            "–†–∞–¥ –ø–æ–º–æ—á—å –≤–∞–º!",
            "–ü–æ–º–æ—â–Ω–∏–∫ –±–∞–Ω–∫–∞ –ë–∞–∫–∞–π:",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!"
        ]
        
        cleaned = answer
        for phrase in unwanted_phrases:
            cleaned = cleaned.replace(phrase, "")
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned
    
    def get_database_stats(self) -> Dict[str, any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            collection = self.vectorstore._collection.get()
            documents = collection.get('documents', [])
            
            return {
                'total_documents': len(documents),
                'faq_count': len(self.faq_database),
                'database_ready': len(documents) > 0
            }
        except Exception as e:
            return {
                'error': str(e),
                'total_documents': 0,
                'database_ready': False
            }